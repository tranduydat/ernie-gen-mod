-----------  Configuration Arguments -----------
attention_probs_dropout_prob: -1.0
batch_size: 1
beam_size: 5
checkpoints: ./checkpoints
continuous_position: True
decr_every_n_nan_or_inf: 2
decr_ratio: 0.8
dev_set: ./datasets/squad_qg//dev.tsv
do_decode: True
do_lower_case: True
do_pred: False
do_test: False
do_train: False
do_val: True
epoch: 10
ernie_config_path: ernie_gen_base/ernie_config.json
eval_mertrics: Bleu_4,METEOR,ROUGE_L
eval_script: bash ./eval/tasks/squad_qg/eval.sh
hidden_dropout_prob: 0.1
in_tokens: False
incr_every_n_steps: 100
incr_ratio: 2.0
init_checkpoint: None
init_loss_scaling: 128.0
init_pretraining_params: ernie_gen_base/params
is_distributed: True
label_smooth: 0.1
learning_rate: 5e-05
length_penalty: 1.0
lr_scheduler: linear_warmup_decay
max_dec_len: 48
max_seq_len: 512
max_src_len: 512
max_tgt_len: 96
noise_prob: 0.7
num_iteration_per_drop_scope: 1
pred_batch_size: 0
pred_set: ./datasets/squad_qg//
random_noise: True
random_seed: 7421
role_type_size: 0
save_and_valid_by_epoch: True
save_steps: 10000
skip_steps: 10
src_do_lower_case: True
src_tokenizer: FullTokenizer
src_vocab_path: None
stream_job: None
task_type: normal
test_set: ./datasets/squad_qg//test.tsv
tgt_type_id: 3
tokenized_input: True
tokenizer: FullTokenizer
train_set: ./datasets/squad_qg//train.tsv
turn_type_size: 0
use_cuda: True
use_dynamic_loss_scaling: False
use_fast_executor: True
use_fp16: False
use_multi_gpu_test: True
validation_steps: 1000
verbose: True
vocab_path: ernie_gen_base/vocab.txt
warmup_proportion: 0.1
weight_decay: 0.01
weight_sharing: True
------------------------------------------------
attention_probs_dropout_prob: 0.1
hidden_act: gelu
hidden_dropout_prob: 0.1
hidden_size: 768
initializer_range: 0.02
intermediate_size: 3072
max_position_embeddings: 1024
num_attention_heads: 12
num_hidden_layers: 12
type_vocab_size: 4
vocab_size: 30522
------------------------------------------------
args.is_distributed: True
worker_endpoints:['172.17.0.5:6170', '172.17.0.5:6171'] trainers_num:2 current_endpoint:172.17.0.5:6171               trainer_id:1


API is deprecated since 2.0.0 Please use FleetAPI instead.
WIKI: https://github.com/PaddlePaddle/Fleet/blob/develop/markdown_doc/transpiler

        
W0507 05:22:37.828338  6074 device_context.cc:237] Please NOTE: device: 1, CUDA Capability: 75, Driver API Version: 11.1, Runtime API Version: 10.0
W0507 05:22:37.830096  6074 device_context.cc:245] device: 1, cuDNN Version: 7.6.
I0507 05:22:38.546034  6168 grpc_server.cc:477] Server listening on 172.17.0.5:6171 successful, selected port: 6171
I0507 05:22:41.911448  6074 rpc_server.cc:28] RPCServer ShutDown 
W0507 05:22:41.911677  6172 grpc_server.cc:604] CompletionQueue RequestSend shutdown!
W0507 05:22:41.911677  6175 grpc_server.cc:604] CompletionQueue RequestSend shutdown!
W0507 05:22:41.911679  6174 grpc_server.cc:604] CompletionQueue RequestSend shutdown!
W0507 05:22:41.911680  6171 grpc_server.cc:604] CompletionQueue RequestSend shutdown!
W0507 05:22:41.911679  6173 grpc_server.cc:604] CompletionQueue RequestSend shutdown!
Load model from ernie_gen_base/params
Traceback (most recent call last):
  File "./run_seq2seq.py", line 310, in <module>
    main(args)
  File "./run_seq2seq.py", line 306, in main
    evaluate(suffix=suffix, do_pred=True)
  File "./run_seq2seq.py", line 44, in evaluate_datasets
    evaluate_dataset("dev", args.dev_set)
  File "./run_seq2seq.py", line 40, in evaluate_dataset
    pyreader.set_batch_generator(data_generator(input_file=path, phase=phase))
  File "/home/reader/seq2seq_reader.py", line 296, in data_generator
    examples = self._read_tsv(input_file)
  File "/home/reader/seq2seq_reader.py", line 83, in _read_tsv
    with open(input_file, "r") as f:
IOError: [Errno 2] No such file or directory: './datasets/squad_qg//dev.tsv'
-----------  Configuration Arguments -----------
attention_probs_dropout_prob: -1.0
batch_size: 1
beam_size: 5
checkpoints: ./checkpoints
continuous_position: True
decr_every_n_nan_or_inf: 2
decr_ratio: 0.8
dev_set: ./datasets/squad_qg//dev.tsv
do_decode: True
do_lower_case: True
do_pred: False
do_test: False
do_train: False
do_val: True
epoch: 10
ernie_config_path: ernie_gen_base/ernie_config.json
eval_mertrics: Bleu_4,METEOR,ROUGE_L
eval_script: bash ./eval/tasks/squad_qg/eval.sh
hidden_dropout_prob: 0.1
in_tokens: False
incr_every_n_steps: 100
incr_ratio: 2.0
init_checkpoint: None
init_loss_scaling: 128.0
init_pretraining_params: ernie_gen_base/params
is_distributed: True
label_smooth: 0.1
learning_rate: 5e-05
length_penalty: 1.0
lr_scheduler: linear_warmup_decay
max_dec_len: 48
max_seq_len: 512
max_src_len: 512
max_tgt_len: 96
noise_prob: 0.7
num_iteration_per_drop_scope: 1
pred_batch_size: 0
pred_set: ./datasets/squad_qg//
random_noise: True
random_seed: 51210
role_type_size: 0
save_and_valid_by_epoch: True
save_steps: 10000
skip_steps: 10
src_do_lower_case: True
src_tokenizer: FullTokenizer
src_vocab_path: None
stream_job: None
task_type: normal
test_set: ./datasets/squad_qg//test.tsv
tgt_type_id: 3
tokenized_input: True
tokenizer: FullTokenizer
train_set: ./datasets/squad_qg//train.tsv
turn_type_size: 0
use_cuda: True
use_dynamic_loss_scaling: False
use_fast_executor: True
use_fp16: False
use_multi_gpu_test: True
validation_steps: 1000
verbose: True
vocab_path: ernie_gen_base/vocab.txt
warmup_proportion: 0.1
weight_decay: 0.01
weight_sharing: True
------------------------------------------------
attention_probs_dropout_prob: 0.1
hidden_act: gelu
hidden_dropout_prob: 0.1
hidden_size: 768
initializer_range: 0.02
intermediate_size: 3072
max_position_embeddings: 1024
num_attention_heads: 12
num_hidden_layers: 12
type_vocab_size: 4
vocab_size: 30522
------------------------------------------------
args.is_distributed: True
worker_endpoints:['172.17.0.5:6170', '172.17.0.5:6171'] trainers_num:2 current_endpoint:172.17.0.5:6171               trainer_id:1


API is deprecated since 2.0.0 Please use FleetAPI instead.
WIKI: https://github.com/PaddlePaddle/Fleet/blob/develop/markdown_doc/transpiler

        
W0507 05:23:50.490846  6805 device_context.cc:237] Please NOTE: device: 1, CUDA Capability: 75, Driver API Version: 11.1, Runtime API Version: 10.0
W0507 05:23:50.492516  6805 device_context.cc:245] device: 1, cuDNN Version: 7.6.
W0507 05:24:01.290261  6805 operator.cc:181] truncated_gaussian_random raises an exception paddle::memory::allocation::BadAlloc, 

--------------------------------------------
C++ Call Stacks (More useful to developers):
--------------------------------------------
0   std::string paddle::platform::GetTraceBackString<std::string>(std::string&&, char const*, int)
1   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
2   paddle::memory::allocation::AlignedAllocator::AllocateImpl(unsigned long)
3   paddle::memory::allocation::AutoGrowthBestFitAllocator::AllocateImpl(unsigned long)
4   paddle::memory::allocation::Allocator::Allocate(unsigned long)
5   paddle::memory::allocation::RetryAllocator::AllocateImpl(unsigned long)
6   paddle::memory::allocation::AllocatorFacade::Alloc(paddle::platform::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::AllocShared(paddle::platform::Place const&, unsigned long)
8   paddle::memory::AllocShared(paddle::platform::Place const&, unsigned long)
9   paddle::framework::Tensor::mutable_data(paddle::platform::Place const&, paddle::framework::proto::VarType_Type, unsigned long)
10  paddle::operators::GPUTruncatedGaussianRandomKernel<float>::Compute(paddle::framework::ExecutionContext const&) const
11  std::_Function_handler<void (paddle::framework::ExecutionContext const&), paddle::framework::OpKernelRegistrarFunctor<paddle::platform::CUDAPlace, false, 0ul, paddle::operators::GPUTruncatedGaussianRandomKernel<float> >::operator()(char const*, char const*, int) const::{lambda(paddle::framework::ExecutionContext const&)#1}>::_M_invoke(std::_Any_data const&, paddle::framework::ExecutionContext const&)
12  paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&, paddle::framework::RuntimeContext*) const
13  paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&) const
14  paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, paddle::platform::Place const&)
15  paddle::framework::Executor::RunPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, bool, bool, bool)
16  paddle::framework::Executor::Run(paddle::framework::ProgramDesc const&, paddle::framework::Scope*, int, bool, bool, std::vector<std::string, std::allocator<std::string> > const&, bool, bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 1. Cannot allocate 2.250244MB memory on GPU 1, available memory is only 5.500000MB.

Please check whether there is any other process using GPU 1.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 

 at (/paddle/paddle/fluid/memory/allocation/cuda_allocator.cc:69)
/usr/local/lib/python2.7/dist-packages/paddle/fluid/executor.py:789: UserWarning: The following exception is not an EOF exception.
  "The following exception is not an EOF exception.")
Traceback (most recent call last):
  File "./run_seq2seq.py", line 310, in <module>
    main(args)
  File "./run_seq2seq.py", line 184, in main
    exe.run(startup_prog)
  File "/usr/local/lib/python2.7/dist-packages/paddle/fluid/executor.py", line 790, in run
    six.reraise(*sys.exc_info())
  File "/usr/local/lib/python2.7/dist-packages/paddle/fluid/executor.py", line 785, in run
    use_program_cache=use_program_cache)
  File "/usr/local/lib/python2.7/dist-packages/paddle/fluid/executor.py", line 838, in _run_impl
    use_program_cache=use_program_cache)
  File "/usr/local/lib/python2.7/dist-packages/paddle/fluid/executor.py", line 912, in _run_program
    fetch_var_name)
RuntimeError: 

--------------------------------------------
C++ Call Stacks (More useful to developers):
--------------------------------------------
0   std::string paddle::platform::GetTraceBackString<std::string>(std::string&&, char const*, int)
1   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
2   paddle::memory::allocation::AlignedAllocator::AllocateImpl(unsigned long)
3   paddle::memory::allocation::AutoGrowthBestFitAllocator::AllocateImpl(unsigned long)
4   paddle::memory::allocation::Allocator::Allocate(unsigned long)
5   paddle::memory::allocation::RetryAllocator::AllocateImpl(unsigned long)
6   paddle::memory::allocation::AllocatorFacade::Alloc(paddle::platform::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::AllocShared(paddle::platform::Place const&, unsigned long)
8   paddle::memory::AllocShared(paddle::platform::Place const&, unsigned long)
9   paddle::framework::Tensor::mutable_data(paddle::platform::Place const&, paddle::framework::proto::VarType_Type, unsigned long)
10  paddle::operators::GPUTruncatedGaussianRandomKernel<float>::Compute(paddle::framework::ExecutionContext const&) const
11  std::_Function_handler<void (paddle::framework::ExecutionContext const&), paddle::framework::OpKernelRegistrarFunctor<paddle::platform::CUDAPlace, false, 0ul, paddle::operators::GPUTruncatedGaussianRandomKernel<float> >::operator()(char const*, char const*, int) const::{lambda(paddle::framework::ExecutionContext const&)#1}>::_M_invoke(std::_Any_data const&, paddle::framework::ExecutionContext const&)
12  paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&, paddle::framework::RuntimeContext*) const
13  paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&) const
14  paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, paddle::platform::Place const&)
15  paddle::framework::Executor::RunPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, bool, bool, bool)
16  paddle::framework::Executor::Run(paddle::framework::ProgramDesc const&, paddle::framework::Scope*, int, bool, bool, std::vector<std::string, std::allocator<std::string> > const&, bool, bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 1. Cannot allocate 2.250244MB memory on GPU 1, available memory is only 5.500000MB.

Please check whether there is any other process using GPU 1.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 

 at (/paddle/paddle/fluid/memory/allocation/cuda_allocator.cc:69)

-----------  Configuration Arguments -----------
attention_probs_dropout_prob: -1.0
batch_size: 1
beam_size: 5
checkpoints: ./checkpoints
continuous_position: True
decr_every_n_nan_or_inf: 2
decr_ratio: 0.8
dev_set: ./datasets/squad_qg//dev.tsv
do_decode: True
do_lower_case: True
do_pred: False
do_test: False
do_train: False
do_val: True
epoch: 10
ernie_config_path: ernie_gen_base/ernie_config.json
eval_mertrics: Bleu_4,METEOR,ROUGE_L
eval_script: bash ./eval/tasks/squad_qg/eval.sh
hidden_dropout_prob: 0.1
in_tokens: False
incr_every_n_steps: 100
incr_ratio: 2.0
init_checkpoint: None
init_loss_scaling: 128.0
init_pretraining_params: ernie_gen_base/params
is_distributed: True
label_smooth: 0.1
learning_rate: 5e-05
length_penalty: 1.0
lr_scheduler: linear_warmup_decay
max_dec_len: 48
max_seq_len: 512
max_src_len: 512
max_tgt_len: 96
noise_prob: 0.7
num_iteration_per_drop_scope: 1
pred_batch_size: 0
pred_set: ./datasets/squad_qg//
random_noise: True
random_seed: 16310
role_type_size: 0
save_and_valid_by_epoch: True
save_steps: 10000
skip_steps: 10
src_do_lower_case: True
src_tokenizer: FullTokenizer
src_vocab_path: None
stream_job: None
task_type: normal
test_set: ./datasets/squad_qg//test.tsv
tgt_type_id: 3
tokenized_input: True
tokenizer: FullTokenizer
train_set: ./datasets/squad_qg//train.tsv
turn_type_size: 0
use_cuda: True
use_dynamic_loss_scaling: False
use_fast_executor: True
use_fp16: False
use_multi_gpu_test: True
validation_steps: 1000
verbose: True
vocab_path: ernie_gen_base/vocab.txt
warmup_proportion: 0.1
weight_decay: 0.01
weight_sharing: True
------------------------------------------------
attention_probs_dropout_prob: 0.1
hidden_act: gelu
hidden_dropout_prob: 0.1
hidden_size: 768
initializer_range: 0.02
intermediate_size: 3072
max_position_embeddings: 1024
num_attention_heads: 12
num_hidden_layers: 12
type_vocab_size: 4
vocab_size: 30522
------------------------------------------------
args.is_distributed: True
worker_endpoints:['172.17.0.5:6170', '172.17.0.5:6171'] trainers_num:2 current_endpoint:172.17.0.5:6171               trainer_id:1


API is deprecated since 2.0.0 Please use FleetAPI instead.
WIKI: https://github.com/PaddlePaddle/Fleet/blob/develop/markdown_doc/transpiler

        
W0507 05:33:35.230891   470 device_context.cc:237] Please NOTE: device: 1, CUDA Capability: 75, Driver API Version: 11.1, Runtime API Version: 10.0
W0507 05:33:35.232760   470 device_context.cc:245] device: 1, cuDNN Version: 7.6.
I0507 05:33:35.798168   564 grpc_server.cc:477] Server listening on 172.17.0.5:6171 successful, selected port: 6171
I0507 05:33:38.935905   470 rpc_server.cc:28] RPCServer ShutDown 
W0507 05:33:38.936939   571 grpc_server.cc:604] CompletionQueue RequestSend shutdown!
W0507 05:33:38.936957   567 grpc_server.cc:604] CompletionQueue RequestSend shutdown!
W0507 05:33:38.936969   568 grpc_server.cc:604] CompletionQueue RequestSend shutdown!
W0507 05:33:38.936954   569 grpc_server.cc:604] CompletionQueue RequestSend shutdown!
W0507 05:33:38.936982   570 grpc_server.cc:604] CompletionQueue RequestSend shutdown!
Load model from ernie_gen_base/params
-----------  Configuration Arguments -----------
attention_probs_dropout_prob: -1.0
batch_size: 1
beam_size: 5
checkpoints: ./checkpoints
continuous_position: True
decr_every_n_nan_or_inf: 2
decr_ratio: 0.8
dev_set: ./datasets/squad_qg//dev.tsv
do_decode: True
do_lower_case: True
do_pred: False
do_test: False
do_train: False
do_val: True
epoch: 10
ernie_config_path: ernie_gen_base/ernie_config.json
eval_mertrics: Bleu_4,METEOR,ROUGE_L
eval_script: bash ./eval/tasks/squad_qg/eval.sh
hidden_dropout_prob: 0.1
in_tokens: False
incr_every_n_steps: 100
incr_ratio: 2.0
init_checkpoint: None
init_loss_scaling: 128.0
init_pretraining_params: ernie_gen_base/params
is_distributed: True
label_smooth: 0.1
learning_rate: 5e-05
length_penalty: 1.0
lr_scheduler: linear_warmup_decay
max_dec_len: 48
max_seq_len: 512
max_src_len: 512
max_tgt_len: 96
noise_prob: 0.7
num_iteration_per_drop_scope: 1
pred_batch_size: 0
pred_set: ./datasets/squad_qg//
random_noise: True
random_seed: 1848
role_type_size: 0
save_and_valid_by_epoch: True
save_steps: 10000
skip_steps: 10
src_do_lower_case: True
src_tokenizer: FullTokenizer
src_vocab_path: None
stream_job: None
task_type: normal
test_set: ./datasets/squad_qg//test.tsv
tgt_type_id: 3
tokenized_input: True
tokenizer: FullTokenizer
train_set: ./datasets/squad_qg//train.tsv
turn_type_size: 0
use_cuda: True
use_dynamic_loss_scaling: False
use_fast_executor: True
use_fp16: False
use_multi_gpu_test: True
validation_steps: 1000
verbose: True
vocab_path: ernie_gen_base/vocab.txt
warmup_proportion: 0.1
weight_decay: 0.01
weight_sharing: True
------------------------------------------------
attention_probs_dropout_prob: 0.1
hidden_act: gelu
hidden_dropout_prob: 0.1
hidden_size: 768
initializer_range: 0.02
intermediate_size: 3072
max_position_embeddings: 1024
num_attention_heads: 12
num_hidden_layers: 12
type_vocab_size: 4
vocab_size: 30522
------------------------------------------------
args.is_distributed: True
worker_endpoints:['172.17.0.5:6170', '172.17.0.5:6171'] trainers_num:2 current_endpoint:172.17.0.5:6171               trainer_id:1


API is deprecated since 2.0.0 Please use FleetAPI instead.
WIKI: https://github.com/PaddlePaddle/Fleet/blob/develop/markdown_doc/transpiler

        
W0507 05:43:11.964701  4853 device_context.cc:237] Please NOTE: device: 1, CUDA Capability: 75, Driver API Version: 11.1, Runtime API Version: 10.0
W0507 05:43:11.966440  4853 device_context.cc:245] device: 1, cuDNN Version: 7.6.
I0507 05:43:12.801954  4947 grpc_server.cc:477] Server listening on 172.17.0.5:6171 successful, selected port: 6171
I0507 05:43:15.869720  4853 rpc_server.cc:28] RPCServer ShutDown 
W0507 05:43:15.869961  4954 grpc_server.cc:604] CompletionQueue RequestSend shutdown!
W0507 05:43:15.869961  4951 grpc_server.cc:604] CompletionQueue RequestSend shutdown!
W0507 05:43:15.869961  4952 grpc_server.cc:604] CompletionQueue RequestSend shutdown!
W0507 05:43:15.869964  4950 grpc_server.cc:604] CompletionQueue RequestSend shutdown!
W0507 05:43:15.869961  4953 grpc_server.cc:604] CompletionQueue RequestSend shutdown!
Load model from ernie_gen_base/params
2021-05-07 05:43:16,745-WARNING: Your reader has raised an exception!
Exception in thread Thread-1:
Traceback (most recent call last):
  File "/usr/lib/python2.7/threading.py", line 801, in __bootstrap_inner
    self.run()
  File "/usr/lib/python2.7/threading.py", line 754, in run
    self.__target(*self.__args, **self.__kwargs)
  File "/usr/local/lib/python2.7/dist-packages/paddle/fluid/reader.py", line 805, in __thread_main__
    six.reraise(*sys.exc_info())
  File "/usr/local/lib/python2.7/dist-packages/paddle/fluid/reader.py", line 785, in __thread_main__
    for tensors in self._tensor_reader():
  File "/home/reader/seq2seq_reader.py", line 314, in wrapper
    examples, batch_size, phase=phase, do_decode=do_decode, place=place):
  File "/home/reader/seq2seq_reader.py", line 278, in _prepare_batch_data
    yield self._pad_batch_records(batch_records, do_decode, place)
  File "/home/reader/seq2seq_reader.py", line 359, in _pad_batch_records
    init_score = self._to_lodtensor(init_score, place, lods)
  File "/home/reader/seq2seq_reader.py", line 328, in _to_lodtensor
    data_tensor.set(data, place)
EnforceNotMet: 

--------------------------------------------
C++ Call Stacks (More useful to developers):
--------------------------------------------
0   std::string paddle::platform::GetTraceBackString<char const*>(char const*&&, char const*, int)
1   paddle::platform::EnforceNotMet::EnforceNotMet(std::__exception_ptr::exception_ptr, char const*, int)
2   paddle::platform::GpuMemcpySync(void*, void const*, unsigned long, cudaMemcpyKind)

----------------------
Error Message Summary:
----------------------
Error: cudaMemcpy failed in paddle::platform::GpuMemcpySync (0x7fce30002050 -> 0x7fce34000000, length: 4) error code : 2, Please see detail in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038: out of memory at (/paddle/paddle/fluid/platform/gpu_info.cc:324)


/usr/local/lib/python2.7/dist-packages/paddle/fluid/executor.py:789: UserWarning: The following exception is not an EOF exception.
  "The following exception is not an EOF exception.")
Traceback (most recent call last):
  File "./run_seq2seq.py", line 310, in <module>
    main(args)
  File "./run_seq2seq.py", line 306, in main
    evaluate(suffix=suffix, do_pred=True)
  File "./run_seq2seq.py", line 44, in evaluate_datasets
    evaluate_dataset("dev", args.dev_set)
  File "./run_seq2seq.py", line 41, in evaluate_dataset
    eval_func(eval_phase="%s_%s" % (phase, suffix), features=reader.get_features(phase))
  File "/home/finetune/seq2seq.py", line 373, in evaluate
    return_numpy=return_numpy)
  File "/usr/local/lib/python2.7/dist-packages/paddle/fluid/executor.py", line 790, in run
    six.reraise(*sys.exc_info())
  File "/usr/local/lib/python2.7/dist-packages/paddle/fluid/executor.py", line 785, in run
    use_program_cache=use_program_cache)
  File "/usr/local/lib/python2.7/dist-packages/paddle/fluid/executor.py", line 838, in _run_impl
    use_program_cache=use_program_cache)
  File "/usr/local/lib/python2.7/dist-packages/paddle/fluid/executor.py", line 912, in _run_program
    fetch_var_name)
paddle.fluid.core_avx.EnforceNotMet: 

--------------------------------------------
C++ Call Stacks (More useful to developers):
--------------------------------------------
0   std::string paddle::platform::GetTraceBackString<std::string const&>(std::string const&, char const*, int)
1   paddle::platform::EnforceNotMet::EnforceNotMet(std::string const&, char const*, int)
2   paddle::operators::reader::BlockingQueue<std::vector<paddle::framework::LoDTensor, std::allocator<paddle::framework::LoDTensor> > >::Receive(std::vector<paddle::framework::LoDTensor, std::allocator<paddle::framework::LoDTensor> >*)
3   paddle::operators::reader::PyReader::ReadNext(std::vector<paddle::framework::LoDTensor, std::allocator<paddle::framework::LoDTensor> >*)
4   std::_Function_handler<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> (), std::__future_base::_Task_setter<std::unique_ptr<std::__future_base::_Result<unsigned long>, std::__future_base::_Result_base::_Deleter>, unsigned long> >::_M_invoke(std::_Any_data const&)
5   std::__future_base::_State_base::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>&, bool&)
6   ThreadPool::ThreadPool(unsigned long)::{lambda()#1}::operator()() const

------------------------------------------
Python Call Stacks (More useful to users):
------------------------------------------
  File "/usr/local/lib/python2.7/dist-packages/paddle/fluid/framework.py", line 2525, in append_op
    attrs=kwargs.get("attrs", None))
  File "/usr/local/lib/python2.7/dist-packages/paddle/fluid/reader.py", line 733, in _init_non_iterable
    outputs={'Out': self._feed_list})
  File "/usr/local/lib/python2.7/dist-packages/paddle/fluid/reader.py", line 646, in __init__
    self._init_non_iterable()
  File "/usr/local/lib/python2.7/dist-packages/paddle/fluid/reader.py", line 280, in from_generator
    iterable, return_list)
  File "/home/finetune/seq2seq.py", line 202, in infilling_decode
    pyreader = fluid.io.DataLoader.from_generator(feed_list=inputs, capacity=50, iterable=False)
  File "/home/finetune/seq2seq.py", line 126, in create_model
    return self.infilling_decode()
  File "./run_seq2seq.py", line 155, in main
    test_pyreader, test_graph_vars = ernie_gen.create_model(decoding=args.do_decode)
  File "./run_seq2seq.py", line 310, in <module>
    main(args)

----------------------
Error Message Summary:
----------------------
Error: Blocking queue is killed because the data reader raises an exception
  [Hint: Expected killed_ != true, but received killed_:1 == true:1.] at (/paddle/paddle/fluid/operators/reader/blocking_queue.h:141)
  [operator < read > error]
-----------  Configuration Arguments -----------
attention_probs_dropout_prob: -1.0
batch_size: 1
beam_size: 5
checkpoints: ./checkpoints
continuous_position: True
decr_every_n_nan_or_inf: 2
decr_ratio: 0.8
dev_set: ./datasets/squad_qg//dev.tsv
do_decode: True
do_lower_case: True
do_pred: False
do_test: False
do_train: False
do_val: True
epoch: 10
ernie_config_path: ernie_gen_base/ernie_config.json
eval_mertrics: Bleu_4,METEOR,ROUGE_L
eval_script: bash ./eval/tasks/squad_qg/eval.sh
hidden_dropout_prob: 0.1
in_tokens: False
incr_every_n_steps: 100
incr_ratio: 2.0
init_checkpoint: None
init_loss_scaling: 128.0
init_pretraining_params: ernie_gen_base/params
is_distributed: True
label_smooth: 0.1
learning_rate: 5e-05
length_penalty: 1.0
lr_scheduler: linear_warmup_decay
max_dec_len: 48
max_seq_len: 512
max_src_len: 512
max_tgt_len: 96
noise_prob: 0.7
num_iteration_per_drop_scope: 1
pred_batch_size: 0
pred_set: ./datasets/squad_qg//
random_noise: True
random_seed: 24814
role_type_size: 0
save_and_valid_by_epoch: True
save_steps: 10000
skip_steps: 10
src_do_lower_case: True
src_tokenizer: FullTokenizer
src_vocab_path: None
stream_job: None
task_type: normal
test_set: ./datasets/squad_qg//test.tsv
tgt_type_id: 3
tokenized_input: True
tokenizer: FullTokenizer
train_set: ./datasets/squad_qg//train.tsv
turn_type_size: 0
use_cuda: True
use_dynamic_loss_scaling: False
use_fast_executor: True
use_fp16: False
use_multi_gpu_test: True
validation_steps: 1000
verbose: True
vocab_path: ernie_gen_base/vocab.txt
warmup_proportion: 0.1
weight_decay: 0.01
weight_sharing: True
------------------------------------------------
attention_probs_dropout_prob: 0.1
hidden_act: gelu
hidden_dropout_prob: 0.1
hidden_size: 768
initializer_range: 0.02
intermediate_size: 3072
max_position_embeddings: 1024
num_attention_heads: 12
num_hidden_layers: 12
type_vocab_size: 4
vocab_size: 30522
------------------------------------------------
args.is_distributed: True
worker_endpoints:['172.17.0.5:6170', '172.17.0.5:6171'] trainers_num:2 current_endpoint:172.17.0.5:6171               trainer_id:1


API is deprecated since 2.0.0 Please use FleetAPI instead.
WIKI: https://github.com/PaddlePaddle/Fleet/blob/develop/markdown_doc/transpiler

        
W0507 05:44:37.710256   509 device_context.cc:237] Please NOTE: device: 1, CUDA Capability: 75, Driver API Version: 11.1, Runtime API Version: 10.0
W0507 05:44:37.711958   509 device_context.cc:245] device: 1, cuDNN Version: 7.6.
I0507 05:44:38.285197   613 grpc_server.cc:477] Server listening on 172.17.0.5:6171 successful, selected port: 6171
I0507 05:44:41.566730   509 rpc_server.cc:28] RPCServer ShutDown 
W0507 05:44:41.567787   616 grpc_server.cc:604] CompletionQueue RequestSend shutdown!
W0507 05:44:41.567788   619 grpc_server.cc:604] CompletionQueue RequestSend shutdown!
W0507 05:44:41.567788   617 grpc_server.cc:604] CompletionQueue RequestSend shutdown!
W0507 05:44:41.567790   618 grpc_server.cc:604] CompletionQueue RequestSend shutdown!
W0507 05:44:41.567790   620 grpc_server.cc:604] CompletionQueue RequestSend shutdown!
Load model from ernie_gen_base/params
-----------  Configuration Arguments -----------
attention_probs_dropout_prob: -1.0
batch_size: 1
beam_size: 5
checkpoints: ./checkpoints
continuous_position: True
decr_every_n_nan_or_inf: 2
decr_ratio: 0.8
dev_set: ./datasets/squad_qg//dev.tsv
do_decode: True
do_lower_case: True
do_pred: False
do_test: False
do_train: False
do_val: True
epoch: 10
ernie_config_path: ernie_gen_base/ernie_config.json
eval_mertrics: Bleu_4,METEOR,ROUGE_L
eval_script: bash ./eval/tasks/squad_qg/eval.sh
hidden_dropout_prob: 0.1
in_tokens: False
incr_every_n_steps: 100
incr_ratio: 2.0
init_checkpoint: None
init_loss_scaling: 128.0
init_pretraining_params: ernie_gen_base/params
is_distributed: True
label_smooth: 0.1
learning_rate: 5e-05
length_penalty: 1.0
lr_scheduler: linear_warmup_decay
max_dec_len: 48
max_seq_len: 512
max_src_len: 512
max_tgt_len: 96
noise_prob: 0.7
num_iteration_per_drop_scope: 1
pred_batch_size: 0
pred_set: ./datasets/squad_qg//
random_noise: True
random_seed: 42669
role_type_size: 0
save_and_valid_by_epoch: True
save_steps: 10000
skip_steps: 10
src_do_lower_case: True
src_tokenizer: FullTokenizer
src_vocab_path: None
stream_job: None
task_type: normal
test_set: ./datasets/squad_qg//test.tsv
tgt_type_id: 3
tokenized_input: True
tokenizer: FullTokenizer
train_set: ./datasets/squad_qg//train.tsv
turn_type_size: 0
use_cuda: True
use_dynamic_loss_scaling: False
use_fast_executor: True
use_fp16: False
use_multi_gpu_test: True
validation_steps: 1000
verbose: True
vocab_path: ernie_gen_base/vocab.txt
warmup_proportion: 0.1
weight_decay: 0.01
weight_sharing: True
------------------------------------------------
attention_probs_dropout_prob: 0.1
hidden_act: gelu
hidden_dropout_prob: 0.1
hidden_size: 768
initializer_range: 0.02
intermediate_size: 3072
max_position_embeddings: 1024
num_attention_heads: 12
num_hidden_layers: 12
type_vocab_size: 4
vocab_size: 30522
------------------------------------------------
args.is_distributed: True
worker_endpoints:['172.17.0.5:6170', '172.17.0.5:6171'] trainers_num:2 current_endpoint:172.17.0.5:6171               trainer_id:1


API is deprecated since 2.0.0 Please use FleetAPI instead.
WIKI: https://github.com/PaddlePaddle/Fleet/blob/develop/markdown_doc/transpiler

        
W0507 05:48:44.917624  2601 device_context.cc:237] Please NOTE: device: 1, CUDA Capability: 75, Driver API Version: 11.1, Runtime API Version: 10.0
W0507 05:48:44.919414  2601 device_context.cc:245] device: 1, cuDNN Version: 7.6.
I0507 05:48:45.493376  2695 grpc_server.cc:477] Server listening on 172.17.0.5:6171 successful, selected port: 6171
I0507 05:48:48.526006  2601 rpc_server.cc:28] RPCServer ShutDown 
W0507 05:48:48.526227  2698 grpc_server.cc:604] CompletionQueue RequestSend shutdown!
W0507 05:48:48.526226  2699 grpc_server.cc:604] CompletionQueue RequestSend shutdown!
W0507 05:48:48.526227  2700 grpc_server.cc:604] CompletionQueue RequestSend shutdown!
W0507 05:48:48.526226  2701 grpc_server.cc:604] CompletionQueue RequestSend shutdown!
W0507 05:48:48.526232  2702 grpc_server.cc:604] CompletionQueue RequestSend shutdown!
Load model from ernie_gen_base/params
