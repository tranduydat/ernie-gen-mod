-----------  Configuration Arguments -----------
attention_probs_dropout_prob: -1.0
batch_size: 1
beam_size: 5
checkpoints: ./checkpoints
continuous_position: True
decr_every_n_nan_or_inf: 2
decr_ratio: 0.8
dev_set: ./datasets/squad_qg//dev.tsv
do_decode: True
do_lower_case: True
do_pred: False
do_test: False
do_train: False
do_val: True
epoch: 10
ernie_config_path: ernie_gen_base/ernie_config.json
eval_mertrics: Bleu_4,METEOR,ROUGE_L
eval_script: bash ./eval/tasks/squad_qg/eval.sh
hidden_dropout_prob: 0.1
in_tokens: False
incr_every_n_steps: 100
incr_ratio: 2.0
init_checkpoint: None
init_loss_scaling: 128.0
init_pretraining_params: ernie_gen_base/params
is_distributed: True
label_smooth: 0.1
learning_rate: 5e-05
length_penalty: 1.0
lr_scheduler: linear_warmup_decay
max_dec_len: 48
max_seq_len: 512
max_src_len: 512
max_tgt_len: 96
noise_prob: 0.7
num_iteration_per_drop_scope: 1
pred_batch_size: 0
pred_set: ./datasets/squad_qg//
random_noise: True
random_seed: 7421
role_type_size: 0
save_and_valid_by_epoch: True
save_steps: 10000
skip_steps: 10
src_do_lower_case: True
src_tokenizer: FullTokenizer
src_vocab_path: None
stream_job: None
task_type: normal
test_set: ./datasets/squad_qg//test.tsv
tgt_type_id: 3
tokenized_input: True
tokenizer: FullTokenizer
train_set: ./datasets/squad_qg//train.tsv
turn_type_size: 0
use_cuda: True
use_dynamic_loss_scaling: False
use_fast_executor: True
use_fp16: False
use_multi_gpu_test: True
validation_steps: 1000
verbose: True
vocab_path: ernie_gen_base/vocab.txt
warmup_proportion: 0.1
weight_decay: 0.01
weight_sharing: True
------------------------------------------------
attention_probs_dropout_prob: 0.1
hidden_act: gelu
hidden_dropout_prob: 0.1
hidden_size: 768
initializer_range: 0.02
intermediate_size: 3072
max_position_embeddings: 1024
num_attention_heads: 12
num_hidden_layers: 12
type_vocab_size: 4
vocab_size: 30522
------------------------------------------------
args.is_distributed: True
worker_endpoints:['172.17.0.5:6170', '172.17.0.5:6171'] trainers_num:2 current_endpoint:172.17.0.5:6170               trainer_id:0


API is deprecated since 2.0.0 Please use FleetAPI instead.
WIKI: https://github.com/PaddlePaddle/Fleet/blob/develop/markdown_doc/transpiler

        
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
W0507 05:22:40.973178  6073 device_context.cc:237] Please NOTE: device: 0, CUDA Capability: 75, Driver API Version: 11.1, Runtime API Version: 10.0
W0507 05:22:40.974941  6073 device_context.cc:245] device: 0, cuDNN Version: 7.6.
26189229c4a6:6073:6073 [0] NCCL INFO NET/Socket : Using [0]eth0:172.17.0.5<0>
26189229c4a6:6073:6073 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

26189229c4a6:6073:6073 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
I0507 05:22:41.909775  6073 rpc_client.h:107] init rpc client with trainer_id 0
Load model from ernie_gen_base/params
Traceback (most recent call last):
  File "./run_seq2seq.py", line 310, in <module>
    main(args)
  File "./run_seq2seq.py", line 306, in main
    evaluate(suffix=suffix, do_pred=True)
  File "./run_seq2seq.py", line 44, in evaluate_datasets
    evaluate_dataset("dev", args.dev_set)
  File "./run_seq2seq.py", line 40, in evaluate_dataset
    pyreader.set_batch_generator(data_generator(input_file=path, phase=phase))
  File "/home/reader/seq2seq_reader.py", line 296, in data_generator
    examples = self._read_tsv(input_file)
  File "/home/reader/seq2seq_reader.py", line 83, in _read_tsv
    with open(input_file, "r") as f:
IOError: [Errno 2] No such file or directory: './datasets/squad_qg//dev.tsv'
-----------  Configuration Arguments -----------
attention_probs_dropout_prob: -1.0
batch_size: 1
beam_size: 5
checkpoints: ./checkpoints
continuous_position: True
decr_every_n_nan_or_inf: 2
decr_ratio: 0.8
dev_set: ./datasets/squad_qg//dev.tsv
do_decode: True
do_lower_case: True
do_pred: False
do_test: False
do_train: False
do_val: True
epoch: 10
ernie_config_path: ernie_gen_base/ernie_config.json
eval_mertrics: Bleu_4,METEOR,ROUGE_L
eval_script: bash ./eval/tasks/squad_qg/eval.sh
hidden_dropout_prob: 0.1
in_tokens: False
incr_every_n_steps: 100
incr_ratio: 2.0
init_checkpoint: None
init_loss_scaling: 128.0
init_pretraining_params: ernie_gen_base/params
is_distributed: True
label_smooth: 0.1
learning_rate: 5e-05
length_penalty: 1.0
lr_scheduler: linear_warmup_decay
max_dec_len: 48
max_seq_len: 512
max_src_len: 512
max_tgt_len: 96
noise_prob: 0.7
num_iteration_per_drop_scope: 1
pred_batch_size: 0
pred_set: ./datasets/squad_qg//
random_noise: True
random_seed: 51210
role_type_size: 0
save_and_valid_by_epoch: True
save_steps: 10000
skip_steps: 10
src_do_lower_case: True
src_tokenizer: FullTokenizer
src_vocab_path: None
stream_job: None
task_type: normal
test_set: ./datasets/squad_qg//test.tsv
tgt_type_id: 3
tokenized_input: True
tokenizer: FullTokenizer
train_set: ./datasets/squad_qg//train.tsv
turn_type_size: 0
use_cuda: True
use_dynamic_loss_scaling: False
use_fast_executor: True
use_fp16: False
use_multi_gpu_test: True
validation_steps: 1000
verbose: True
vocab_path: ernie_gen_base/vocab.txt
warmup_proportion: 0.1
weight_decay: 0.01
weight_sharing: True
------------------------------------------------
attention_probs_dropout_prob: 0.1
hidden_act: gelu
hidden_dropout_prob: 0.1
hidden_size: 768
initializer_range: 0.02
intermediate_size: 3072
max_position_embeddings: 1024
num_attention_heads: 12
num_hidden_layers: 12
type_vocab_size: 4
vocab_size: 30522
------------------------------------------------
args.is_distributed: True
worker_endpoints:['172.17.0.5:6170', '172.17.0.5:6171'] trainers_num:2 current_endpoint:172.17.0.5:6170               trainer_id:0


API is deprecated since 2.0.0 Please use FleetAPI instead.
WIKI: https://github.com/PaddlePaddle/Fleet/blob/develop/markdown_doc/transpiler

        
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
-----------  Configuration Arguments -----------
attention_probs_dropout_prob: -1.0
batch_size: 1
beam_size: 5
checkpoints: ./checkpoints
continuous_position: True
decr_every_n_nan_or_inf: 2
decr_ratio: 0.8
dev_set: ./datasets/squad_qg//dev.tsv
do_decode: True
do_lower_case: True
do_pred: False
do_test: False
do_train: False
do_val: True
epoch: 10
ernie_config_path: ernie_gen_base/ernie_config.json
eval_mertrics: Bleu_4,METEOR,ROUGE_L
eval_script: bash ./eval/tasks/squad_qg/eval.sh
hidden_dropout_prob: 0.1
in_tokens: False
incr_every_n_steps: 100
incr_ratio: 2.0
init_checkpoint: None
init_loss_scaling: 128.0
init_pretraining_params: ernie_gen_base/params
is_distributed: True
label_smooth: 0.1
learning_rate: 5e-05
length_penalty: 1.0
lr_scheduler: linear_warmup_decay
max_dec_len: 48
max_seq_len: 512
max_src_len: 512
max_tgt_len: 96
noise_prob: 0.7
num_iteration_per_drop_scope: 1
pred_batch_size: 0
pred_set: ./datasets/squad_qg//
random_noise: True
random_seed: 16310
role_type_size: 0
save_and_valid_by_epoch: True
save_steps: 10000
skip_steps: 10
src_do_lower_case: True
src_tokenizer: FullTokenizer
src_vocab_path: None
stream_job: None
task_type: normal
test_set: ./datasets/squad_qg//test.tsv
tgt_type_id: 3
tokenized_input: True
tokenizer: FullTokenizer
train_set: ./datasets/squad_qg//train.tsv
turn_type_size: 0
use_cuda: True
use_dynamic_loss_scaling: False
use_fast_executor: True
use_fp16: False
use_multi_gpu_test: True
validation_steps: 1000
verbose: True
vocab_path: ernie_gen_base/vocab.txt
warmup_proportion: 0.1
weight_decay: 0.01
weight_sharing: True
------------------------------------------------
attention_probs_dropout_prob: 0.1
hidden_act: gelu
hidden_dropout_prob: 0.1
hidden_size: 768
initializer_range: 0.02
intermediate_size: 3072
max_position_embeddings: 1024
num_attention_heads: 12
num_hidden_layers: 12
type_vocab_size: 4
vocab_size: 30522
------------------------------------------------
args.is_distributed: True
worker_endpoints:['172.17.0.5:6170', '172.17.0.5:6171'] trainers_num:2 current_endpoint:172.17.0.5:6170               trainer_id:0


API is deprecated since 2.0.0 Please use FleetAPI instead.
WIKI: https://github.com/PaddlePaddle/Fleet/blob/develop/markdown_doc/transpiler

        
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
W0507 05:33:38.248224   469 device_context.cc:237] Please NOTE: device: 0, CUDA Capability: 75, Driver API Version: 11.1, Runtime API Version: 10.0
W0507 05:33:38.249840   469 device_context.cc:245] device: 0, cuDNN Version: 7.6.
26189229c4a6:469:469 [0] NCCL INFO NET/Socket : Using [0]eth0:172.17.0.5<0>
26189229c4a6:469:469 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

26189229c4a6:469:469 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
I0507 05:33:38.927206   469 rpc_client.h:107] init rpc client with trainer_id 0
Load model from ernie_gen_base/params
-----------  Configuration Arguments -----------
attention_probs_dropout_prob: -1.0
batch_size: 1
beam_size: 5
checkpoints: ./checkpoints
continuous_position: True
decr_every_n_nan_or_inf: 2
decr_ratio: 0.8
dev_set: ./datasets/squad_qg//dev.tsv
do_decode: True
do_lower_case: True
do_pred: False
do_test: False
do_train: False
do_val: True
epoch: 10
ernie_config_path: ernie_gen_base/ernie_config.json
eval_mertrics: Bleu_4,METEOR,ROUGE_L
eval_script: bash ./eval/tasks/squad_qg/eval.sh
hidden_dropout_prob: 0.1
in_tokens: False
incr_every_n_steps: 100
incr_ratio: 2.0
init_checkpoint: None
init_loss_scaling: 128.0
init_pretraining_params: ernie_gen_base/params
is_distributed: True
label_smooth: 0.1
learning_rate: 5e-05
length_penalty: 1.0
lr_scheduler: linear_warmup_decay
max_dec_len: 48
max_seq_len: 512
max_src_len: 512
max_tgt_len: 96
noise_prob: 0.7
num_iteration_per_drop_scope: 1
pred_batch_size: 0
pred_set: ./datasets/squad_qg//
random_noise: True
random_seed: 1848
role_type_size: 0
save_and_valid_by_epoch: True
save_steps: 10000
skip_steps: 10
src_do_lower_case: True
src_tokenizer: FullTokenizer
src_vocab_path: None
stream_job: None
task_type: normal
test_set: ./datasets/squad_qg//test.tsv
tgt_type_id: 3
tokenized_input: True
tokenizer: FullTokenizer
train_set: ./datasets/squad_qg//train.tsv
turn_type_size: 0
use_cuda: True
use_dynamic_loss_scaling: False
use_fast_executor: True
use_fp16: False
use_multi_gpu_test: True
validation_steps: 1000
verbose: True
vocab_path: ernie_gen_base/vocab.txt
warmup_proportion: 0.1
weight_decay: 0.01
weight_sharing: True
------------------------------------------------
attention_probs_dropout_prob: 0.1
hidden_act: gelu
hidden_dropout_prob: 0.1
hidden_size: 768
initializer_range: 0.02
intermediate_size: 3072
max_position_embeddings: 1024
num_attention_heads: 12
num_hidden_layers: 12
type_vocab_size: 4
vocab_size: 30522
------------------------------------------------
args.is_distributed: True
worker_endpoints:['172.17.0.5:6170', '172.17.0.5:6171'] trainers_num:2 current_endpoint:172.17.0.5:6170               trainer_id:0


API is deprecated since 2.0.0 Please use FleetAPI instead.
WIKI: https://github.com/PaddlePaddle/Fleet/blob/develop/markdown_doc/transpiler

        
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
W0507 05:43:14.952440  4852 device_context.cc:237] Please NOTE: device: 0, CUDA Capability: 75, Driver API Version: 11.1, Runtime API Version: 10.0
W0507 05:43:14.954380  4852 device_context.cc:245] device: 0, cuDNN Version: 7.6.
26189229c4a6:4852:4852 [0] NCCL INFO NET/Socket : Using [0]eth0:172.17.0.5<0>
26189229c4a6:4852:4852 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

26189229c4a6:4852:4852 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
I0507 05:43:15.867981  4852 rpc_client.h:107] init rpc client with trainer_id 0
Load model from ernie_gen_base/params
W0507 05:43:28.170538  4852 operator.cc:181] concat raises an exception paddle::memory::allocation::BadAlloc, 

--------------------------------------------
C++ Call Stacks (More useful to developers):
--------------------------------------------
0   std::string paddle::platform::GetTraceBackString<std::string>(std::string&&, char const*, int)
1   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
2   paddle::memory::allocation::AlignedAllocator::AllocateImpl(unsigned long)
3   paddle::memory::allocation::AutoGrowthBestFitAllocator::AllocateImpl(unsigned long)
4   paddle::memory::allocation::Allocator::Allocate(unsigned long)
5   paddle::memory::allocation::RetryAllocator::AllocateImpl(unsigned long)
6   paddle::memory::allocation::AllocatorFacade::Alloc(paddle::platform::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::AllocShared(paddle::platform::Place const&, unsigned long)
8   paddle::memory::AllocShared(paddle::platform::Place const&, unsigned long)
9   paddle::framework::Tensor::mutable_data(paddle::platform::Place const&, paddle::framework::proto::VarType_Type, unsigned long)
10  paddle::operators::ConcatKernel<paddle::platform::CUDADeviceContext, float>::Compute(paddle::framework::ExecutionContext const&) const
11  std::_Function_handler<void (paddle::framework::ExecutionContext const&), paddle::framework::OpKernelRegistrarFunctor<paddle::platform::CUDAPlace, false, 1ul, paddle::operators::ConcatKernel<paddle::platform::CUDADeviceContext, double>, paddle::operators::ConcatKernel<paddle::platform::CUDADeviceContext, float>, paddle::operators::ConcatKernel<paddle::platform::CUDADeviceContext, paddle::platform::float16>, paddle::operators::ConcatKernel<paddle::platform::CUDADeviceContext, long>, paddle::operators::ConcatKernel<paddle::platform::CUDADeviceContext, int> >::operator()(char const*, char const*, int) const::{lambda(paddle::framework::ExecutionContext const&)#1}>::_M_invoke(std::_Any_data const&, paddle::framework::ExecutionContext const&)
12  paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&, paddle::framework::RuntimeContext*) const
13  paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&) const
14  paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, paddle::platform::Place const&)
15  paddle::framework::Executor::RunPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, bool, bool, bool)
16  paddle::operators::WhileOp::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&) const
17  paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, paddle::platform::Place const&)
18  paddle::framework::Executor::RunPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, bool, bool, bool)
19  paddle::framework::Executor::Run(paddle::framework::ProgramDesc const&, paddle::framework::Scope*, int, bool, bool, std::vector<std::string, std::allocator<std::string> > const&, bool, bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 2.827393MB memory on GPU 0, available memory is only 21.937500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 

 at (/paddle/paddle/fluid/memory/allocation/cuda_allocator.cc:69)
W0507 05:43:28.172061  4852 operator.cc:181] while raises an exception paddle::memory::allocation::BadAlloc, 

--------------------------------------------
C++ Call Stacks (More useful to developers):
--------------------------------------------
0   std::string paddle::platform::GetTraceBackString<std::string>(std::string&&, char const*, int)
1   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
2   paddle::memory::allocation::AlignedAllocator::AllocateImpl(unsigned long)
3   paddle::memory::allocation::AutoGrowthBestFitAllocator::AllocateImpl(unsigned long)
4   paddle::memory::allocation::Allocator::Allocate(unsigned long)
5   paddle::memory::allocation::RetryAllocator::AllocateImpl(unsigned long)
6   paddle::memory::allocation::AllocatorFacade::Alloc(paddle::platform::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::AllocShared(paddle::platform::Place const&, unsigned long)
8   paddle::memory::AllocShared(paddle::platform::Place const&, unsigned long)
9   paddle::framework::Tensor::mutable_data(paddle::platform::Place const&, paddle::framework::proto::VarType_Type, unsigned long)
10  paddle::operators::ConcatKernel<paddle::platform::CUDADeviceContext, float>::Compute(paddle::framework::ExecutionContext const&) const
11  std::_Function_handler<void (paddle::framework::ExecutionContext const&), paddle::framework::OpKernelRegistrarFunctor<paddle::platform::CUDAPlace, false, 1ul, paddle::operators::ConcatKernel<paddle::platform::CUDADeviceContext, double>, paddle::operators::ConcatKernel<paddle::platform::CUDADeviceContext, float>, paddle::operators::ConcatKernel<paddle::platform::CUDADeviceContext, paddle::platform::float16>, paddle::operators::ConcatKernel<paddle::platform::CUDADeviceContext, long>, paddle::operators::ConcatKernel<paddle::platform::CUDADeviceContext, int> >::operator()(char const*, char const*, int) const::{lambda(paddle::framework::ExecutionContext const&)#1}>::_M_invoke(std::_Any_data const&, paddle::framework::ExecutionContext const&)
12  paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&, paddle::framework::RuntimeContext*) const
13  paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&) const
14  paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, paddle::platform::Place const&)
15  paddle::framework::Executor::RunPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, bool, bool, bool)
16  paddle::operators::WhileOp::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&) const
17  paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, paddle::platform::Place const&)
18  paddle::framework::Executor::RunPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, bool, bool, bool)
19  paddle::framework::Executor::Run(paddle::framework::ProgramDesc const&, paddle::framework::Scope*, int, bool, bool, std::vector<std::string, std::allocator<std::string> > const&, bool, bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 2.827393MB memory on GPU 0, available memory is only 21.937500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 

 at (/paddle/paddle/fluid/memory/allocation/cuda_allocator.cc:69)
/usr/local/lib/python2.7/dist-packages/paddle/fluid/executor.py:789: UserWarning: The following exception is not an EOF exception.
  "The following exception is not an EOF exception.")
Traceback (most recent call last):
  File "./run_seq2seq.py", line 310, in <module>
    main(args)
  File "./run_seq2seq.py", line 306, in main
    evaluate(suffix=suffix, do_pred=True)
  File "./run_seq2seq.py", line 44, in evaluate_datasets
    evaluate_dataset("dev", args.dev_set)
  File "./run_seq2seq.py", line 41, in evaluate_dataset
    eval_func(eval_phase="%s_%s" % (phase, suffix), features=reader.get_features(phase))
  File "/home/finetune/seq2seq.py", line 373, in evaluate
    return_numpy=return_numpy)
  File "/usr/local/lib/python2.7/dist-packages/paddle/fluid/executor.py", line 790, in run
    six.reraise(*sys.exc_info())
  File "/usr/local/lib/python2.7/dist-packages/paddle/fluid/executor.py", line 785, in run
    use_program_cache=use_program_cache)
  File "/usr/local/lib/python2.7/dist-packages/paddle/fluid/executor.py", line 838, in _run_impl
    use_program_cache=use_program_cache)
  File "/usr/local/lib/python2.7/dist-packages/paddle/fluid/executor.py", line 912, in _run_program
    fetch_var_name)
RuntimeError: 

--------------------------------------------
C++ Call Stacks (More useful to developers):
--------------------------------------------
0   std::string paddle::platform::GetTraceBackString<std::string>(std::string&&, char const*, int)
1   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
2   paddle::memory::allocation::AlignedAllocator::AllocateImpl(unsigned long)
3   paddle::memory::allocation::AutoGrowthBestFitAllocator::AllocateImpl(unsigned long)
4   paddle::memory::allocation::Allocator::Allocate(unsigned long)
5   paddle::memory::allocation::RetryAllocator::AllocateImpl(unsigned long)
6   paddle::memory::allocation::AllocatorFacade::Alloc(paddle::platform::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::AllocShared(paddle::platform::Place const&, unsigned long)
8   paddle::memory::AllocShared(paddle::platform::Place const&, unsigned long)
9   paddle::framework::Tensor::mutable_data(paddle::platform::Place const&, paddle::framework::proto::VarType_Type, unsigned long)
10  paddle::operators::ConcatKernel<paddle::platform::CUDADeviceContext, float>::Compute(paddle::framework::ExecutionContext const&) const
11  std::_Function_handler<void (paddle::framework::ExecutionContext const&), paddle::framework::OpKernelRegistrarFunctor<paddle::platform::CUDAPlace, false, 1ul, paddle::operators::ConcatKernel<paddle::platform::CUDADeviceContext, double>, paddle::operators::ConcatKernel<paddle::platform::CUDADeviceContext, float>, paddle::operators::ConcatKernel<paddle::platform::CUDADeviceContext, paddle::platform::float16>, paddle::operators::ConcatKernel<paddle::platform::CUDADeviceContext, long>, paddle::operators::ConcatKernel<paddle::platform::CUDADeviceContext, int> >::operator()(char const*, char const*, int) const::{lambda(paddle::framework::ExecutionContext const&)#1}>::_M_invoke(std::_Any_data const&, paddle::framework::ExecutionContext const&)
12  paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&, paddle::framework::RuntimeContext*) const
13  paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&) const
14  paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, paddle::platform::Place const&)
15  paddle::framework::Executor::RunPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, bool, bool, bool)
16  paddle::operators::WhileOp::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&) const
17  paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, paddle::platform::Place const&)
18  paddle::framework::Executor::RunPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, bool, bool, bool)
19  paddle::framework::Executor::Run(paddle::framework::ProgramDesc const&, paddle::framework::Scope*, int, bool, bool, std::vector<std::string, std::allocator<std::string> > const&, bool, bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 2.827393MB memory on GPU 0, available memory is only 21.937500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 

 at (/paddle/paddle/fluid/memory/allocation/cuda_allocator.cc:69)

-----------  Configuration Arguments -----------
attention_probs_dropout_prob: -1.0
batch_size: 1
beam_size: 5
checkpoints: ./checkpoints
continuous_position: True
decr_every_n_nan_or_inf: 2
decr_ratio: 0.8
dev_set: ./datasets/squad_qg//dev.tsv
do_decode: True
do_lower_case: True
do_pred: False
do_test: False
do_train: False
do_val: True
epoch: 10
ernie_config_path: ernie_gen_base/ernie_config.json
eval_mertrics: Bleu_4,METEOR,ROUGE_L
eval_script: bash ./eval/tasks/squad_qg/eval.sh
hidden_dropout_prob: 0.1
in_tokens: False
incr_every_n_steps: 100
incr_ratio: 2.0
init_checkpoint: None
init_loss_scaling: 128.0
init_pretraining_params: ernie_gen_base/params
is_distributed: True
label_smooth: 0.1
learning_rate: 5e-05
length_penalty: 1.0
lr_scheduler: linear_warmup_decay
max_dec_len: 48
max_seq_len: 512
max_src_len: 512
max_tgt_len: 96
noise_prob: 0.7
num_iteration_per_drop_scope: 1
pred_batch_size: 0
pred_set: ./datasets/squad_qg//
random_noise: True
random_seed: 24814
role_type_size: 0
save_and_valid_by_epoch: True
save_steps: 10000
skip_steps: 10
src_do_lower_case: True
src_tokenizer: FullTokenizer
src_vocab_path: None
stream_job: None
task_type: normal
test_set: ./datasets/squad_qg//test.tsv
tgt_type_id: 3
tokenized_input: True
tokenizer: FullTokenizer
train_set: ./datasets/squad_qg//train.tsv
turn_type_size: 0
use_cuda: True
use_dynamic_loss_scaling: False
use_fast_executor: True
use_fp16: False
use_multi_gpu_test: True
validation_steps: 1000
verbose: True
vocab_path: ernie_gen_base/vocab.txt
warmup_proportion: 0.1
weight_decay: 0.01
weight_sharing: True
------------------------------------------------
attention_probs_dropout_prob: 0.1
hidden_act: gelu
hidden_dropout_prob: 0.1
hidden_size: 768
initializer_range: 0.02
intermediate_size: 3072
max_position_embeddings: 1024
num_attention_heads: 12
num_hidden_layers: 12
type_vocab_size: 4
vocab_size: 30522
------------------------------------------------
args.is_distributed: True
worker_endpoints:['172.17.0.5:6170', '172.17.0.5:6171'] trainers_num:2 current_endpoint:172.17.0.5:6170               trainer_id:0


API is deprecated since 2.0.0 Please use FleetAPI instead.
WIKI: https://github.com/PaddlePaddle/Fleet/blob/develop/markdown_doc/transpiler

        
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
W0507 05:44:40.737207   508 device_context.cc:237] Please NOTE: device: 0, CUDA Capability: 75, Driver API Version: 11.1, Runtime API Version: 10.0
W0507 05:44:40.738994   508 device_context.cc:245] device: 0, cuDNN Version: 7.6.
26189229c4a6:508:508 [0] NCCL INFO NET/Socket : Using [0]eth0:172.17.0.5<0>
26189229c4a6:508:508 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

26189229c4a6:508:508 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
I0507 05:44:41.558033   508 rpc_client.h:107] init rpc client with trainer_id 0
Load model from ernie_gen_base/params
sort: multi-character tab '$\t'
Traceback (most recent call last):
  File "./run_seq2seq.py", line 310, in <module>
    main(args)
  File "./run_seq2seq.py", line 306, in main
    evaluate(suffix=suffix, do_pred=True)
  File "./run_seq2seq.py", line 44, in evaluate_datasets
    evaluate_dataset("dev", args.dev_set)
  File "./run_seq2seq.py", line 41, in evaluate_dataset
    eval_func(eval_phase="%s_%s" % (phase, suffix), features=reader.get_features(phase))
  File "/home/finetune/seq2seq.py", line 448, in evaluate
    phase=eval_phase.split("_")[0], features=features)
  File "/home/eval/gen_eval.py", line 65, in eval
    raise Exception("Eval mertric: %s is not supported" % mertric)
Exception: Eval mertric: Bleu_4 is not supported
-----------  Configuration Arguments -----------
attention_probs_dropout_prob: -1.0
batch_size: 1
beam_size: 5
checkpoints: ./checkpoints
continuous_position: True
decr_every_n_nan_or_inf: 2
decr_ratio: 0.8
dev_set: ./datasets/squad_qg//dev.tsv
do_decode: True
do_lower_case: True
do_pred: False
do_test: False
do_train: False
do_val: True
epoch: 10
ernie_config_path: ernie_gen_base/ernie_config.json
eval_mertrics: Bleu_4,METEOR,ROUGE_L
eval_script: bash ./eval/tasks/squad_qg/eval.sh
hidden_dropout_prob: 0.1
in_tokens: False
incr_every_n_steps: 100
incr_ratio: 2.0
init_checkpoint: None
init_loss_scaling: 128.0
init_pretraining_params: ernie_gen_base/params
is_distributed: True
label_smooth: 0.1
learning_rate: 5e-05
length_penalty: 1.0
lr_scheduler: linear_warmup_decay
max_dec_len: 48
max_seq_len: 512
max_src_len: 512
max_tgt_len: 96
noise_prob: 0.7
num_iteration_per_drop_scope: 1
pred_batch_size: 0
pred_set: ./datasets/squad_qg//
random_noise: True
random_seed: 42669
role_type_size: 0
save_and_valid_by_epoch: True
save_steps: 10000
skip_steps: 10
src_do_lower_case: True
src_tokenizer: FullTokenizer
src_vocab_path: None
stream_job: None
task_type: normal
test_set: ./datasets/squad_qg//test.tsv
tgt_type_id: 3
tokenized_input: True
tokenizer: FullTokenizer
train_set: ./datasets/squad_qg//train.tsv
turn_type_size: 0
use_cuda: True
use_dynamic_loss_scaling: False
use_fast_executor: True
use_fp16: False
use_multi_gpu_test: True
validation_steps: 1000
verbose: True
vocab_path: ernie_gen_base/vocab.txt
warmup_proportion: 0.1
weight_decay: 0.01
weight_sharing: True
------------------------------------------------
attention_probs_dropout_prob: 0.1
hidden_act: gelu
hidden_dropout_prob: 0.1
hidden_size: 768
initializer_range: 0.02
intermediate_size: 3072
max_position_embeddings: 1024
num_attention_heads: 12
num_hidden_layers: 12
type_vocab_size: 4
vocab_size: 30522
------------------------------------------------
args.is_distributed: True
worker_endpoints:['172.17.0.5:6170', '172.17.0.5:6171'] trainers_num:2 current_endpoint:172.17.0.5:6170               trainer_id:0


API is deprecated since 2.0.0 Please use FleetAPI instead.
WIKI: https://github.com/PaddlePaddle/Fleet/blob/develop/markdown_doc/transpiler

        
server not ready, wait 3 sec to retry...
not ready endpoints:['172.17.0.5:6171']
W0507 05:48:47.926762  2600 device_context.cc:237] Please NOTE: device: 0, CUDA Capability: 75, Driver API Version: 11.1, Runtime API Version: 10.0
W0507 05:48:47.928476  2600 device_context.cc:245] device: 0, cuDNN Version: 7.6.
26189229c4a6:2600:2600 [0] NCCL INFO NET/Socket : Using [0]eth0:172.17.0.5<0>
26189229c4a6:2600:2600 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

26189229c4a6:2600:2600 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
I0507 05:48:48.524297  2600 rpc_client.h:107] init rpc client with trainer_id 0
Load model from ernie_gen_base/params
sort: multi-character tab '$\t'
Traceback (most recent call last):
  File "./run_seq2seq.py", line 310, in <module>
    main(args)
  File "./run_seq2seq.py", line 306, in main
    evaluate(suffix=suffix, do_pred=True)
  File "./run_seq2seq.py", line 44, in evaluate_datasets
    evaluate_dataset("dev", args.dev_set)
  File "./run_seq2seq.py", line 41, in evaluate_dataset
    eval_func(eval_phase="%s_%s" % (phase, suffix), features=reader.get_features(phase))
  File "/home/finetune/seq2seq.py", line 449, in evaluate
    phase=eval_phase.split("_")[0], features=features)
  File "/home/eval/gen_eval.py", line 65, in eval
    raise Exception("Eval mertric: %s is not supported" % mertric)
Exception: Eval mertric: Bleu_4 is not supported
